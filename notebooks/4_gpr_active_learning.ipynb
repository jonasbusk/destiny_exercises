{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active learning with Gaussian Process regression (GPR) model\n",
    "\n",
    "In this exercise we will build on what we learned in the previous exercise by applying the Gaussian process regression (GPR) model in an active learning setting.\n",
    "\n",
    "In active learning we iteratively improve a model by using the model to query for additional training data. To select the data that can improve the model the most, we utilize uncertainty about the predictions. The idea is that by selecting the data the model is most uncertain about and adding it to the training dataset, we can reduce uncertainty an thus improve the overall performance of the model. This is especially useful when labeling data is expensive and we want to be smart about what data we choose to label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "First we import the dependencies.\n",
    "\n",
    "If you are in Colab, you need to install the [GPyTorch](https://gpytorch.ai/) package by uncommenting and running the line `!pip3 install gpytorch` below before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "# !pip3 install gpytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import gpytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Again we consider synthetic data generated by the Schwefel function.\n",
    "\n",
    "We first visualize the function on a grid of input points and then we sample the initial training dataset with a small amount of additive observation noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schwefel(x):\n",
    "    \"\"\"The Schwefel function has many local optima.\"\"\"\n",
    "    return 418.9829 * x.shape[-1] - (x * torch.sin(torch.sqrt(torch.abs(x)))).sum(dim=-1)\n",
    "\n",
    "def noisy_schwefel(x, noise_std=1.0):\n",
    "    \"\"\"The Schwefel function with observation noise.\"\"\"\n",
    "    return schwefel(x) + noise_std * torch.randn(x.shape[0])\n",
    "\n",
    "def standardize(y):\n",
    "    \"\"\"Standardize a vector to have zero mean and unit standard deviation.\"\"\"\n",
    "    return (y - y.mean()) / y.std()\n",
    "\n",
    "# Define a grid of points on which to evaluate the function\n",
    "n_grid = 100\n",
    "levels = 30\n",
    "x_min = torch.tensor([0, 0])\n",
    "x_max = torch.tensor([430, 430])\n",
    "\n",
    "x0 = torch.linspace(0, 1, n_grid)\n",
    "x1 = torch.linspace(0, 1, n_grid)\n",
    "g0, g1 = torch.meshgrid(x0, x1, indexing=\"xy\")\n",
    "x_grid = torch.stack((g0.reshape(-1), g1.reshape(-1)), 1)\n",
    "\n",
    "y_grid = schwefel(x_grid * (x_max - x_min) + x_min)\n",
    "\n",
    "vmin, vmax = y_grid.min(), y_grid.max()\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.title(\"Schwefel function\")\n",
    "plt.contourf(x0.numpy(), x1.numpy(), y_grid.reshape(n_grid, n_grid).numpy(), vmin=vmin, vmax=vmax, levels=levels)\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a training set and plot it\n",
    "n_train = 50\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x_train = torch.rand(n_train, 2)\n",
    "y_train = noisy_schwefel(x_train * (x_max - x_min) + x_min)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.title('Training data')\n",
    "plt.scatter(x_train[:,0], x_train[:,1], c=y_train, vmin=vmin, vmax=vmax)\n",
    "plt.colorbar()\n",
    "plt.xlim(0, 1); plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We can use the same simple GPyTorch GPR model we used in the previous exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to train the model multiple times, we define a helper function that takes data as input and returns a trained model. \n",
    "We likewise define a function to make predictions with a trained model. \n",
    "Finally we define functions to compute error metrics to evaluate the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gpr(x_train, y_train, training_iter=100):\n",
    "    \"\"\"Train a Gaussian process regression model.\"\"\"\n",
    "    # Initialize likelihood and model\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(x_train, standardize(y_train), likelihood)\n",
    "    # Training mode\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "    # Loss function - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    # Training loop\n",
    "    losses = []\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(x_train)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, standardize(y_train))\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "    return model, likelihood, losses\n",
    "\n",
    "\n",
    "def predict(model, likelihood, x):\n",
    "    \"\"\"Predict using a Gaussian process regression model.\"\"\"\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        f_pred = model(x)  # model posterior distribution\n",
    "        y_pred = likelihood(f_pred)  # posterior predictive distribution\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    \"\"\"Compute mean squared error.\"\"\"\n",
    "    return torch.mean((y_true - y_pred)**2)\n",
    "\n",
    "\n",
    "def r2(y_true, y_pred):\n",
    "    \"\"\"Compute coefficient of determination.\"\"\"\n",
    "    ssr = torch.sum((y_true - y_pred)**2)\n",
    "    sst = torch.sum((y_true - torch.mean(y_true))**2)\n",
    "    return 1 - (ssr / sst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed to improve the model with active learning, let us see how the model performs on the initial training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, likelihood, losses = train_gpr(x_train, y_train)\n",
    "y_pred = predict(model, likelihood, x_grid)\n",
    "y_pred_mean = y_pred.mean\n",
    "y_pred_var = y_pred.variance\n",
    "\n",
    "# plt.figure(figsize=(6,3))\n",
    "# plt.title(\"Training loss\")\n",
    "# plt.plot(losses)\n",
    "# plt.xlabel(\"Iteration\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.title(\"Prediction mean\")\n",
    "plt.contourf(x0.numpy(), x1.numpy(), y_pred_mean.reshape(n_grid, n_grid).numpy(), levels=levels)\n",
    "plt.scatter(x_train[:,0], x_train[:,1], c=standardize(y_train))  # plot training data\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.title(\"Prediction uncertainty\")\n",
    "plt.contourf(x0.numpy(), x1.numpy(), torch.sqrt(y_pred_var).reshape(n_grid, n_grid).numpy(), levels=levels)\n",
    "plt.scatter(x_train[:,0], x_train[:,1], c=\"black\")  # plot training data\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see if we can improve the model. The code below runs an active learning loop where in each iteration we:\n",
    "* Train a model on the current training dataset. \n",
    "* Use the trained model to make predictions on a pool of data (here we use the grid data).\n",
    "* Select the data point with the highest predicted uncertainty.\n",
    "* Label the new data point with the Schwefel function.\n",
    "* Add the new data point to the training dataset.\n",
    "* Evaluate the model on a test dataset and compute error metrics.\n",
    "* Repeat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active learning loop\n",
    "\n",
    "def run_active_learning_loop(x_data, y_data, x_pool, x_test, y_test, n_steps=10):\n",
    "    \"\"\"Run the active learning loop.\"\"\"\n",
    "    mse_list, r2_list = [], []\n",
    "    for i in range(n_steps):\n",
    "        print(f\"Step: {i+1}/{n_steps}\")\n",
    "        # Train GPR model\n",
    "        model, likelihood, losses = train_gpr(x_data, y_data)\n",
    "        # Predict on new data\n",
    "        y_pred = predict(model, likelihood, x_pool)\n",
    "        # Select most uncertain data point\n",
    "        index = y_pred.variance.argmax()\n",
    "        x_new = x_pool[index].unsqueeze(0)\n",
    "        # Label the new data point\n",
    "        y_new = noisy_schwefel(x_new * (x_max - x_min) + x_min)\n",
    "        # Add the new data point to the dataset\n",
    "        x_data = torch.cat([x_data, x_new])\n",
    "        y_data = torch.cat([y_data, y_new])\n",
    "        # Evaluate the model on the test dataset and save the results\n",
    "        y_pred = predict(model, likelihood, x_test)\n",
    "        mse_list.append(mse(standardize(y_test), y_pred.mean))\n",
    "        r2_list.append(r2(standardize(y_test), y_pred.mean))\n",
    "    return x_data, y_data, mse_list, r2_list\n",
    "\n",
    "\n",
    "# Run the active learning loop\n",
    "active_learning_steps = 20\n",
    "x_data, y_data, mse_list, r2_list = run_active_learning_loop(\n",
    "    x_train.clone(),\n",
    "    y_train.clone(),\n",
    "    x_grid,\n",
    "    x_grid,\n",
    "    y_grid,\n",
    "    n_steps=active_learning_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if the model improved, we can plot the error metrics from each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot errors\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.title(\"MSE\")\n",
    "plt.plot(mse_list)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.title(\"R^2\")\n",
    "plt.plot(r2_list)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"R^2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the final dataset and the model predictions. What do you observe? Has the model improved? Can we improve it even more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data size:\", x_data.shape, y_data.shape)\n",
    "\n",
    "model, likelihood, losses = train_gpr(x_data, y_data)\n",
    "y_pred = predict(model, likelihood, x_grid)\n",
    "y_pred_mean = y_pred.mean\n",
    "y_pred_var = y_pred.variance\n",
    "\n",
    "# plt.figure(figsize=(6,3))\n",
    "# plt.title(\"Training loss\")\n",
    "# plt.plot(losses)\n",
    "# plt.xlabel(\"Iteration\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.title('Data')\n",
    "plt.scatter(x_data[:,0], x_data[:,1], c=y_data, vmin=vmin, vmax=vmax)\n",
    "plt.colorbar()\n",
    "plt.xlim(0, 1); plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.title(\"Prediction mean\")\n",
    "plt.contourf(x0.numpy(), x1.numpy(), y_pred_mean.reshape(n_grid, n_grid).numpy(), levels=levels)\n",
    "plt.scatter(x_data[:,0], x_data[:,1], c=standardize(y_data))  # plot training data\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.title(\"Prediction uncertainty\")\n",
    "plt.contourf(x0.numpy(), x1.numpy(), torch.sqrt(y_pred_var).reshape(n_grid, n_grid).numpy(), levels=levels)\n",
    "plt.scatter(x_data[:,0], x_data[:,1], c=\"black\")  # plot training data\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Exercises:\n",
    "\n",
    "* Try to increase the number of active learning iterations. Can you further reduce the error and uncertainty?\n",
    "* Try to reduce the number of initial training data points. How does it affect the final dataset and results?\n",
    "* Try to change the input range by changing `x_min` and `x_max` to create a more complicated function.\n",
    "* How would you create an algorithm that selects a batch of diverse new training points (instead of just a single point) in each iteration?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
